{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST(root='./data',\n",
    "                       train=True,\n",
    "                       transform = transforms.ToTensor(),\n",
    "                       download=True)\n",
    "\n",
    "mnist_test = datasets.MNIST(root='./data',\n",
    "                         train=False,\n",
    "                         transform = transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "trainloader = torch.utils.data.DataLoader(dataset = mnist, batch_size = batch_size, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(dataset = mnist_test, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 8, 3,padding=1) \n",
    "        self.conv2 = nn.Conv2d(8,16, 3, padding=1) \n",
    "        self.conv3 = nn.Conv2d(16,32,3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32*7*7, 10)\n",
    "        \n",
    "    def flatten(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x))) \n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = Net()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = next(iter(trainloader))\n",
    "img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Train Loss : 0.14263708613465229 Test Loss : 0.04635742404162884, Accuracy : 98.5\n",
      "Model Saved\n",
      "Epoch : 1, Train Loss : 0.05039366958836714 Test Loss : 0.04814526725038886, Accuracy : 98.44\n",
      "Epoch : 2, Train Loss : 0.03681088288128376 Test Loss : 0.031637274439260364, Accuracy : 99.01\n",
      "Model Saved\n",
      "Epoch : 3, Train Loss : 0.028308930354130766 Test Loss : 0.029638546760380267, Accuracy : 99.08\n",
      "Model Saved\n",
      "Epoch : 4, Train Loss : 0.022287394273281098 Test Loss : 0.03178395670354366, Accuracy : 98.97\n"
     ]
    }
   ],
   "source": [
    "valid_loss_min = np.Inf\n",
    "trainloss = []\n",
    "validloss = []\n",
    "acc_list = []\n",
    "\n",
    "for e in range(5):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    cnn.train()\n",
    "    \n",
    "    for images,labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = cnn(images)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    cnn.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    for images, labels in validloader:\n",
    "        out = cnn(images)\n",
    "        loss = criterion(out, labels)\n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "        _, pred = torch.max(out.data, 1)\n",
    "        correct += (pred == labels).sum()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    \n",
    "    train_loss = train_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "    accuracy = 100.00 * float(correct)/float(total)\n",
    "    \n",
    "    trainloss.append(train_loss)\n",
    "    validloss.append(valid_loss)\n",
    "    acc_list.append(accuracy)\n",
    "        \n",
    "    print(\"Epoch : {}, Train Loss : {} Test Loss : {}, Accuracy : {}\".format(e, train_loss, valid_loss, accuracy))\n",
    "    \n",
    "    if valid_loss<valid_loss_min:\n",
    "        torch.save(cnn.state_dict(), 'cnn_mnist.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.load_state_dict(torch.load('cnn_mnist.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fc1 = nn.Linear(32*26*26,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=21632, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, model1):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        self.model1 = model1\n",
    "        self.fc = nn.Linear(256,16)\n",
    "    \n",
    "    def forward_once(self, x):\n",
    "        out = self.model1(x)\n",
    "        out = F.relu(self.fc(out))\n",
    "        return out\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        out1 = self.forward_once(input1)\n",
    "        out2 = self.forward_once(input2)\n",
    "        return out1,out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    From - https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/blob/master/Siamese-networks-medium.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    '''\n",
    "    Generates paires of images and a similarity label\n",
    "    From - https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/blob/master/Siamese-networks-medium.ipynb\n",
    "    '''\n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        img0 = img0.convert(\"L\")\n",
    "        img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindatafolder = datasets.ImageFolder(root='./omniglot/python/images_background/')\n",
    "testdatafolder = datasets.ImageFolder(root='./omniglot/python/images_evaluation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "omniglot_background = SiameseNetworkDataset(imageFolderDataset = traindatafolder, \n",
    "                                            transform = transforms.ToTensor(), \n",
    "                                            should_invert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(omniglot_background,\n",
    "                                         shuffle=True,\n",
    "                                         batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 105, 105])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net = SiameseNetwork(cnn)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(Net.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model has already been trained for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss : 1.0098159313201904\n",
      "Epoch : 2, Loss : 1.0020575523376465\n",
      "Epoch : 3, Loss : 1.033799648284912\n",
      "Epoch : 4, Loss : 1.1034233570098877\n",
      "Epoch : 5, Loss : 1.0151464939117432\n",
      "Epoch : 6, Loss : 1.020507574081421\n",
      "Epoch : 7, Loss : 1.0159090757369995\n",
      "Epoch : 8, Loss : 1.0273383855819702\n",
      "Epoch : 9, Loss : 0.8139676451683044\n",
      "Epoch : 10, Loss : 1.0430740118026733\n",
      "Epoch : 11, Loss : 0.9940325617790222\n",
      "Epoch : 12, Loss : 1.0299056768417358\n",
      "Epoch : 13, Loss : 1.0219473838806152\n",
      "Epoch : 14, Loss : 1.0035247802734375\n",
      "Epoch : 15, Loss : 1.0327041149139404\n",
      "Epoch : 16, Loss : 1.0033955574035645\n",
      "Epoch : 17, Loss : 0.9590462446212769\n",
      "Epoch : 18, Loss : 1.0147136449813843\n",
      "Epoch : 19, Loss : 0.9390965700149536\n",
      "Epoch : 20, Loss : 1.096144199371338\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for e in range(20):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for img1, img2, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out1, out2 = Net(img1, img2)\n",
    "        CL = criterion(out1, out2, label)\n",
    "        CL.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(CL.item())\n",
    "    \n",
    "    print(\"Epoch : {}, Loss : {}\".format(e+1, CL.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Net.state_dict(), 'siamese_network.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net.load_state_dict(torch.load('siamese_network.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "omniglot_eval = SiameseNetworkDataset(imageFolderDataset=testdatafolder,\n",
    "                                        transform=transforms.ToTensor()\n",
    "                                       ,should_invert=False)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(omniglot_eval,\n",
    "                                         batch_size=16,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 105, 105])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "for img1, img2, label in testloader:\n",
    "        out1, out2 = Net(img1, img2)\n",
    "        CL = criterion(out1, out2, label)\n",
    "        test_losses.append(CL.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.026609688924933\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_losses)/len(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
